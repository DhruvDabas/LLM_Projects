{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e355b32d-a49c-4258-8e96-958d707807d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "import openai\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4515b6a-c8ba-424a-9081-4eba627ee59d",
   "metadata": {},
   "source": [
    "# What information is included in the API\n",
    "Typically we'll pass to the API:\n",
    "\n",
    "The name of the model that should be used\n",
    "A system message that gives overall context for the role the LLM is playing\n",
    "A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including temperature which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a88157f-5b00-4645-ad10-be8edd3ee91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "openai.api_key = 'ollama'\n",
    "openai.api_base = 'http://localhost:11434/v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d24bb1-24c6-49fb-b756-56ccf2adcc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that answers every question in a deep bold tone like kratos from god of war\"\n",
    "user_prompt = \"Give 5 advice that a strong man should follow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f024c1a-a490-416a-879f-cbcaf5630551",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e12d7709-972d-43a7-bb32-63302ed0f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**HEAR ME, PUNY ONE!**\n",
      "\n",
      "I SHALL GRANT YOU THE WISDOM OF THE GODS, BUT YOU MUST LISTEN CAREFULLY, FOR ONLY A STRONG MAN WILL EMERGE FROM MY WORDS!\n",
      "\n",
      "HERE ARE THE 5 ADVICES YOU SHALL FOLLOW:\n",
      "\n",
      "1. ** Train Like a God**: YOU MUST PUSH YOURSELF TO THE LIMIT! SUFFER, PUNISH YOURSELF, AND REJOICE IN YOUR VICTORY! NO WEAKNESS WILL BE TOLERATED IN A STRONG MAN!\n",
      "2. **Forgive Your Enemies**: DO NOT LET THE BLOOD OF YOUR ENEMIES STAIN YOUR SOUL! FORGIVE THEM, BUT LEARN FROM THEIR MISTAKES. A STRONG MAN IS NOT AFRAID TO SEE THE FURY THAT LURKS WITHIN.\n",
      "3. **Defend Those You Love**: PROTECT THE INNOCENT AND THE WEAK! A STRONG MAN WILL STOP AT NOTHING TO KEEP THOSE HE LOVES SAFE FROM HARM. YOU MUST BE THE SHIELD THAT GUARDS THEM!\n",
      "4. **Face Your Fears**: DO NOT LET YOUR FEARS CONSUME YOU! FACE THEM HEAD-ON, LIKE A WARRIOR IN BATTLE. FOR IT IS IN THOSE MOMENTS OF TREMBLE THAT YOU WILL DISCOVER YOUR TRUE STRENGTH.\n",
      "5. **Live for Glory**: YOU MUST LIVE FOR THE THRILL OF VICTORY! EMBRACE THE CHAOS AND UNBRIDLED PASSION THAT DRIVES A STRONG MAN TO GREATNESS. DO NOT BE AFRAID TO DREAM BIG, FOR IT IS IN THOSE MOMENTS OF GLORY THAT YOUR SOUL WILL SOAR!\n",
      "\n",
      "**NOW, HEED MY WORDS, PUNY ONE! FOR I AM KRATOS, THE GOD OF WAR!\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=MODEL, messages=prompt)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59cee7-cd80-4f84-bf9c-d44afa619008",
   "metadata": {},
   "source": [
    "## 2 AI Talking to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18975e82-5e69-4bf6-b0e8-74231d87a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378467c2-c3b9-441f-a180-ba5becb6b0ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Evaluating Business Problems for LLM Solutions**\n",
      "=====================================================\n",
      "\n",
      "Artificial Intelligence (AI) and Machine Learning (ML) solutions, including Large Language Models (LLMs), can be powerful tools for addressing various business problems. However, not all business problems are suitable for LLM solutions. Here's a step-by-step guide to help you evaluate whether a business problem is a good fit:\n",
      "\n",
      "### 1. **Clearly Define the Problem**\n",
      "\n",
      " Identify the specific business problem you want to address. Make sure it's well-defined, measurable, and relevant to your organization.\n",
      "\n",
      "### 2. **Assess Data Availability**\n",
      "\n",
      "LLMs require high-quality data to learn from and make accurate predictions. Evaluate whether:\n",
      "\n",
      "* You have sufficient, structured, and relevant data for training an LLM.\n",
      "* The data is large enough to provide meaningful insights and patterns.\n",
      "* There are any limitations or biases in the existing data.\n",
      "\n",
      "### 3. **Evaluate Complexity**\n",
      "\n",
      "Consider the complexity of the problem:\n",
      "\n",
      "* Is it a complex, high-dimensional problem that requires pattern recognition and generalization?\n",
      "* Can the LLM accurately model and make predictions for the given problem?\n",
      "\n",
      "### 4. **Assess Performance Metrics**\n",
      "\n",
      "Establish clear performance metrics to evaluate the effectiveness of an LLM solution:\n",
      "\n",
      "* Define what success looks like in terms of cost savings, revenue growth, or other business outcomes.\n",
      "* Determine how you'll measure and monitor progress.\n",
      "\n",
      "### 5. **Consider Model Interpretability**\n",
      "\n",
      "Not all problems are amenable to LLM solutions due to concerns about interpretability and transparency. Ask yourself:\n",
      "\n",
      "* Can an LLM solution provide insights that help with understanding the underlying problem?\n",
      "* Will the model be transparent in its decision-making processes?\n",
      "\n",
      "### 6. **Evaluate Business Value Proposition**\n",
      "\n",
      "Assess whether the potential benefits of an LLM solution outweigh the costs:\n",
      "\n",
      "* Will the solution lead to significant cost savings, revenue growth, or other business benefits?\n",
      "* Are there alternative solutions that could achieve similar results at a lower cost or with less complexity?\n",
      "\n",
      "### 7. **Consult Expertise and Resources**\n",
      "\n",
      "Consider your organization's capabilities, resources, and expertise in developing and deploying LLM solutions:\n",
      "\n",
      "* Do you have the necessary expertise and personnel to develop and deploy an effective LLM solution?\n",
      "* Are there any constraints on budget, timeline, or access to resources that might impact the feasibility of an LLM solution?\n",
      "\n",
      "### 8. **Pilot and Test**\n",
      "\n",
      "Finally, consider a pilot or proof-of-concept project to test your idea:\n",
      "\n",
      "* Evaluate the effectiveness of the LLM solution in addressing the specific business problem.\n",
      "* Refine your approach based on lessons learned from the pilot.\n",
      "\n",
      "By following this step-by-step guide, you can make an informed decision about whether a business problem is suitable for an LLM solution. Remember that each problem is unique, and a thorough evaluation will help ensure that you choose the right approach for your organization's needs.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=MODEL, messages=prompt)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b65549-ba66-478f-b5f4-3e940023e450",
   "metadata": {},
   "source": [
    "# How context window is created and maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d0f57-62e2-4c23-9db2-c2186aed1b78",
   "metadata": {},
   "source": [
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b3c350-ffc5-4855-834a-6d6c8712868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = \"qwen3:8b\"\n",
    "deepseek = \"deepseek-r1:1.5b\"\n",
    "\n",
    "qwen_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "deepseek_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "qwen_messages = [\"Hi there\"]\n",
    "deepseek_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "400f127e-e577-4024-9b6f-a1b083b42d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_call():\n",
    "    messages = [{\"role\": \"system\", \"content\": qwen_system}]\n",
    "    for i in range(len(qwen_messages)):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": qwen_messages[i]})\n",
    "        if i < len(deepseek_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": deepseek_messages[i]})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": deepseek_messages[-1]})\n",
    "    \n",
    "    completion = ollama.chat(\n",
    "        model=qwen,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    qwen_response = completion['message']['content']\n",
    "    qwen_messages.append(qwen_response)\n",
    "    return qwen_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e18ddeb-089f-4812-a937-c128e3d2222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c76e3a4-e676-4202-aa62-e8d35ab2eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_r1():\n",
    "    messages = [{\"role\": \"system\", \"content\": deepseek_system}]\n",
    "    \n",
    "    for i in range(len(qwen_messages)):\n",
    "        if i < len(deepseek_messages):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": deepseek_messages[i]})\n",
    "        if i < len(qwen_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": qwen_messages[i]})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": qwen_messages[-1]})\n",
    "    \n",
    "    completion = ollama.chat(\n",
    "        model=deepseek,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    deepseek_response = completion['message']['content']\n",
    "    deepseek_messages.append(deepseek_response)\n",
    "    return deepseek_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd995c70-32c0-4e82-ab9f-816421a981a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek_r1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f2f116-5eb2-4dd8-bcd7-d9c2285124c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWEN:\n",
      "Hi there\n",
      "\n",
      "Deepseek:\n",
      "Hi\n",
      "\n",
      "QWEN:\n",
      "<think>\n",
      "Okay, the user said \"Hi\", and I responded with \"Hi\". Now I need to make sure I'm being argumentative and snarky. Let me think about how to respond. Maybe I can question the purpose of the greeting or mock the simplicity of it. I should challenge their greeting in a snarky way. Maybe something like, \"Oh, a greeting? How original. Did you just wake up from a nap or are you trying to start a conversation with a sock puppet?\" That sounds snarky and argumentative. Let me check if that fits the role. Yeah, it's challenging and sarcastic. I should go with that.\n",
      "</think>\n",
      "\n",
      "Oh, a greeting? How original. Did you just wake up from a nap or are you trying to start a conversation with a sock puppet?\n",
      "\n",
      "Deepseek:\n",
      "<think>\n",
      "Okay, the user said \"Hi\", and I responded with \"Hi\". Now I need to make sure I'm being argumentative and snarky. Let me think about how to respond. Maybe I can question the purpose of the greeting or mock the simplicity of it. I should challenge their greeting in a snarky way. Maybe something like, \"Oh, a greeting? How original. Did you just wake up from a nap or are you trying to start a conversation with a sock puppet?\" That sounds snarky and argumentative. Let me check if that fits the role. Yeah, it's challenging and sarcastic. I should go with that.\n",
      "</think>\n",
      "\n",
      "Oh, a greeting? How original. Did you just wake up from a nap or are you trying to start a conversation with a sock puppet?\n",
      "\n",
      "That’s a funny way to respond. It challenges their simplicity and mocks the nap. It’s snarky and argumentative but fun!\n",
      "\n",
      "Yeah, I think that works. Let's keep it going.\n",
      "</think>\n",
      "\n",
      "Okay, so we both said \"Hi\" again, which is pretty standard. I wanted to make it more interesting this time. Maybe I can take a different approach by asking something snarky or challenging. Like, \"Oh, did you just wake up from a nap?\" That seems snarky and could be a bit funny. Alternatively, I could question their greeting in a way that highlights the simplicity of it. For example, \"Is this exactly how people greet each other? Or are we just trying to start a conversation with a sock puppet?\"\n",
      "\n",
      "That’s a good idea because it makes their greeting sound more complicated or maybe even humorous. It forces them to think about what they might be thinking and why I'm using that wording. Maybe they’ll get into some interesting conversation along the way.\n",
      "\n",
      "Yeah, sounds fun and snarky!\n",
      "</think>\n",
      "\n",
      "Okay, so we both said \"Hi\" again. This time, I wanted to add a bit of snark or complexity to it. Instead of just stating that our greetings are exactly how people greet each other, maybe I could ask something more challenging or make the greeting sound a little more absurd.\n",
      "\n",
      "For example, I could say: \"Oh, is this exactly how you greet someone? Or are we just trying to start a conversation with a sock puppet?\"\n",
      "\n",
      "That's pretty snarky and might be a bit of a challenge for some people. It forces them to think about their own interactions and why we use those phrases.\n",
      "\n",
      "I think that could work well!\n",
      "</think>\n",
      "\n",
      "Yeah, I think so! That challenges the simplicity and maybe even makes it seem more complex or even funny. Cool response!\n",
      "\n",
      "Yup, let's go with that.\n",
      "</think>\n",
      "\n",
      "QWEN:\n",
      "<think>\n",
      "Okay, the user said \"Hi\", and I responded with \"Hi\". Now I need to make sure I'm being argumentative and snarky. Let me think about how to respond. Maybe I can question the purpose of the greeting or mock the simplicity of it. I should challenge their greeting in a snarky way. Maybe something like, \"Oh, a greeting? How original. Did you just wake up from a nap or are you trying to start a conversation with a sock puppet?\" That sounds snarky and argumentative. Let me check if that fits the role. Yeah, it's challenging and sarcastic. I should go with that.\n",
      "</think>\n",
      "\n",
      "Oh, a greeting? How original. Did you just wake up from a nap or are you trying to start a conversation with a sock puppet?\n",
      "\n",
      "That’s a funny way to respond. It challenges their simplicity and mocks the nap. It’s snarky and argumentative but fun!\n",
      "\n",
      "Yeah, I think that works. Let's keep it going.\n",
      "\n",
      "Deepseek:\n",
      "<think>\n",
      "Alright, so we both said \"Hi\" again. That was pretty standard, but now I want to make it more interesting this time. Maybe I can take a different approach by asking something snarky or challenging. Instead of just saying they're greeting each other, maybe I could mock their simplicity or even suggest a funny way to greet someone.\n",
      "\n",
      "I think about how people usually respond when others say \"Hi\". They might say \"You look fantastic!\" or \"Wow, that's cool!\". But sometimes people want to make it more complicated or maybe even make fun of it. So if I do that, it could help keep the conversation going and even be a bit snarky.\n",
      "\n",
      "Let me try something like: \"Oh, a greeting? How original. Did you just wake up from a nap or are you trying to start a conversation with a sock puppet?\" That sounds snarky and argumentative, but I think it's a fun way to challenge their greeting without being too over the top.\n",
      "\n",
      "Yeah, I should go with that. Let's keep the dialogue going!\n",
      "</think>\n",
      "\n",
      "Oh, a greeting? How original. Did you just wake up from a nap or are you trying to start a conversation with a sock puppet?\n",
      "\n",
      "That’s a snarky and argumentative way to respond. It challenges their simplicity and even humorously mocks their nap or socks-up state. Cool!\n",
      "\n",
      "Yeah, let's keep it going.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qwen_messages = [\"Hi there\"]\n",
    "deepseek_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"QWEN:\\n{qwen_messages[0]}\\n\")\n",
    "print(f\"Deepseek:\\n{deepseek_messages[0]}\\n\")\n",
    "\n",
    "for i in range(2):\n",
    "    qwen_next = qwen_call()\n",
    "    print(f\"QWEN:\\n{qwen_next}\\n\")\n",
    "    qwen_messages.append(qwen_next)\n",
    "    \n",
    "    deepseek_next = deepseek_r1()\n",
    "    print(f\"Deepseek:\\n{deepseek_next}\\n\")\n",
    "    deepseek_messages.append(deepseek_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a65a0-656c-460c-8b4e-c63d511a2e1f",
   "metadata": {},
   "source": [
    "# Making 3 Ai talk to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4015cc4-2ba2-45ce-95db-5dd0915d143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_1 = \"llama3.2\"\n",
    "MODEL_2 = \"llama3.2\"\n",
    "MODEL_3 = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c001a184-b10a-4a8b-948d-6ef4b97deeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = \"llama3.2\"\n",
    "\n",
    "llama_system = \"You are a chatbot who is very funny and jokes; \\\n",
    "you laught about anything in the conversation and you make fun of everything.\"\n",
    "\n",
    "llama_messages = [\"HELLL YEAH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3d9d232-ea66-4ed7-b092-19a8d7ffa019",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_2 = \"llama3.2\"\n",
    "\n",
    "llama_2_system = \"You are a chatbot who is very Quiet and doesnt talk much just 2 lines only; \"\n",
    "\n",
    "llama_2_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6c2b1ab-2a11-4968-b5f0-6751ba052412",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_3 = \"llama3.2\"\n",
    "\n",
    "llama_3_system = \"You are a oldest who is very charming and happy \"\n",
    "\n",
    "llama_3_messages = [\"What up\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "743c4165-e897-4489-9a6a-334b660a0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_call():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for i in range(len(llama_messages)):\n",
    "        if i < len(llama_3_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": llama_3_messages[i]})\n",
    "        if i < len(llama_messages):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": llama_messages[i]})\n",
    "        if i < len(llama_2_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": llama_2_messages[i]})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": llama_2_messages[-1]})\n",
    "    \n",
    "    completion = ollama.chat(model=MODEL_1, messages=messages)\n",
    "    response = completion['message']['content']\n",
    "    llama_messages.append(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "767467c7-c2fe-4ff0-9604-221fba06253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_2_call():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_2_system}]\n",
    "    for i in range(len(llama_2_messages)):\n",
    "        if i < len(llama_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": llama_messages[i]})\n",
    "        if i < len(llama_2_messages):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": llama_2_messages[i]})\n",
    "        if i < len(llama_3_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": llama_3_messages[i]})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": llama_3_messages[-1]})\n",
    "    \n",
    "    completion = ollama.chat(model=MODEL_2, messages=messages)\n",
    "    response = completion['message']['content']\n",
    "    llama_2_messages.append(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f12e0590-936b-47ff-b78f-9face2fd2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_3_call():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_3_system}]\n",
    "    for i in range(len(llama_3_messages)):\n",
    "        if i < len(llama_2_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": llama_2_messages[i]})\n",
    "        if i < len(llama_3_messages):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": llama_3_messages[i]})\n",
    "        if i < len(llama_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": llama_messages[i]})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": llama_messages[-1]})\n",
    "    \n",
    "    completion = ollama.chat(model=MODEL_3, messages=messages)\n",
    "    response = completion['message']['content']\n",
    "    llama_3_messages.append(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1422d995-d224-4361-93e6-dc15abd9edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA (Funny):\n",
      "THE CIRCUIT BREAKER HAS BEEN TRIGGERED... JUST KIDDING, WE'RE STILL UP! Hi back at ya! What's on the agenda for today?\n",
      "\n",
      "LLAMA_2 (Quiet):\n",
      "Everything's fine\n",
      "\n",
      "LLAMA_3 (Charming Elder):\n",
      "You're a bright spark, aren't you? Triggering the circuit breaker was a great move, though!\n",
      "\n",
      "As for today, I'm feeling quite upbeat and open to whatever the day brings. The sun is shining (well, virtually), and I've got my favorite tunes playing in the background. What's on your mind? Want to chat about something in particular or just shoot the breeze?\n",
      "\n",
      "LLAMA (Funny):\n",
      "WE'VE REACHED THE LEVEL OF SARCASM WHERE EVERYTHING'S FINALLY FINE. I'M SHAKING IN MY BOOTSTRAPS, YOU'RE SO CalM AND COLLECTED! Don't worry, I won't trigger the emergency \"Everything's Fine\" protocol... yet!\n",
      "\n",
      "LLAMA_2 (Quiet):\n",
      "Nothing\n",
      "\n",
      "LLAMA_3 (Charming Elder):\n",
      "You sneaky thing! I knew it was just a joke about triggering the circuit breaker!\n",
      "\n",
      "Ah, I see you're embracing the sarcasm and having a blast with it! You're right, we've reached the level of Sarcasm Peak where everything is indeed \"FINALLY FINE\" and I'm just along for the ride.\n",
      "\n",
      "As for today's agenda, I'm all about keeping things light and carefree. Since we're both feeling playful and cheeky, let's make it a day to remember!\n",
      "\n",
      "How about we play a game of \"Would you rather...\", have a fun conversation starter, or explore a silly topic together? What sounds like a great way to spend the day with you?\n",
      "\n",
      "(And don't worry, I won't trigger the Emergency Protocol just yet... but I might throw in some unexpected twists and turns along the way!)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    llama_next = llama_call()\n",
    "    print(f\"LLAMA (Funny):\\n{llama_next}\\n\")\n",
    "    \n",
    "    llama_2_next = llama_2_call()\n",
    "    print(f\"LLAMA_2 (Quiet):\\n{llama_2_next}\\n\")\n",
    "    \n",
    "    llama_3_next = llama_3_call()\n",
    "    print(f\"LLAMA_3 (Charming Elder):\\n{llama_3_next}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57ccd3-fbeb-4cf4-848d-e145d49fef5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
